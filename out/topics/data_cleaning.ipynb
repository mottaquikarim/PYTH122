{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mottaquikarim/pycontent/blob/master/.out/topics/data_cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "<img src=\"https://i.chzbgr.com/full/1898496256/h42C0CC42/panda-cleaning-instructions\" style=\"margin: 0 auto; float: right;\"/>\n",
    "\n",
    "Data cleaning is arguably as important as any amount of insight you obtain from your dataset. The more data there is, especially data aggregated from multiple sources, the messier it is. You need to reformat and standardize it before you can successfully complete any real analysis. Otherwise...garbage in, garbage out...\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Vectorized typecasting\n",
    "* Scaling variables\n",
    "* Dropping null values\n",
    "* Element-wise functions with .map()\n",
    "* Element-wise functions with .apply()\n",
    "* Row- & Column-wise functions with .apply()\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load the data with `imdbID` as the index and make a copy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb_orig = pd.read_csv('https://raw.githubusercontent.com/mottaquikarim/pycontent/master/content/raw_data/omdb4500_cleaning.csv', index_col='imdbID')\n",
    "movies = omdb_orig.copy()\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Element-wise Functions with `.map()`\n",
    "\n",
    "An **elementwise** function is one that you call on a Series object as a whole, but that vectorizes the functions actions across each of the Series elements. \n",
    "\n",
    "### Typecasting\n",
    "\n",
    "Typecasting a Series is one of the most basic elementwise functions. Most commonly in cleaning your data, you'll use:\n",
    "\n",
    "* `pd.to_numeric(s)`: typecast the items in a Series to ints or floats; will infer which numeric type is best\n",
    "* `s.astype()`: typecast the items in a Series to some data type; accepts `'int64'`, `'float64'`, `'str'`, etc.\n",
    "\n",
    "Let's test these out quickly on the `Year` column. What data type is it now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = movies['Year'].copy()\n",
    "print(type(test_year[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convert it to string type using `.astype()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = test_year.astype('str')\n",
    "type(test_year[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convert it to one of the numeric types using `pd.to_numeric(s)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = pd.to_numeric('int64')\n",
    "type(test_year[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The `.map()` function\n",
    "\n",
    "For the next few example, we'll leverage the `s.map(arg, na_action=None)` function, another **elementwise** function. You can use the `.map(arg, na_action=None)` function to substitute or transform each value in a Series with another value. `.map()` itself serves to pass along \"instructions\" for how to manipulate each element in the Series. Accordingly, the `arg` parameter will accept single-argument functions, dicts, or Series. As you might imagine, `.map()` requires us to pass it a \"mapping\" for the before and after values.\n",
    "\n",
    "\n",
    "| Type of `arg` |    Map From   |    Map To    |\n",
    "|:-------------:|:-------------:|:------------:|\n",
    "|   Function    |  1 Parameter  | Return Value |\n",
    "|     Dict      |      Key      |     Value    |\n",
    "|    Series     |     Index     |     Value    |\n",
    "\n",
    "\n",
    "In *most* cases, if there are null values in the original Series, an error will stop your `.map()` function's execution. (We'll see the exception soon.) The `na_action` parameter allows you to bypass this issue until you decide what how to handle different pieces of missing data in your dataset. If you set `na_action='ignore'`, `.map()` will simply skip over null values.\n",
    "\n",
    "### Mapping Strings to Lists\n",
    "\n",
    "The `Genres`, `Country`, and `Languages` columns often hold more than one value per row. But when we pulled this data from the API, we got the contents of each cell in the form of a single string with the values separated by commas. If we want to evaluate the different categories within each of these variables, we need to break out the individual values from the string into list format.\n",
    "\n",
    "### Genres\n",
    "\n",
    "Count and view the rows with missing `Genres` data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_genre = movies[pd.isnull(movies['Genres'])]\n",
    "print(movies['Genres'].isna().sum())\n",
    "missing_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We could pass `na_action='ignore'`, but since there are only 3, we might as well look them up and fill in the info ourselves. We can check this by making sure the count of nulls afterward is 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_updates = {\n",
    "    'tt8026554': 'Drama',\n",
    "    'tt6215446': 'Comedy, Horror',\n",
    "    'tt10084752': 'Documentary'\n",
    "}\n",
    "\n",
    "for imdbID, genre in genre_updates.items():\n",
    "    movies.loc[imdbID, 'Genres'] = genre\n",
    "\n",
    "print(movies['Genres'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's make a copy of the `Genres` column to operate on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_genre = movies['Genres'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To turn each string into a list, all we have to do is split each string at the commas. But we have to pass `.map()` a function for this, remember? For brevity, whenever possible, most people use **lambda functions** with `.map()`. A **lambda function** is a nameless function that is defined, used, and forgotten in one line. Here's the syntax relative to a regular function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def split_list(x):\n",
    "    return x.split(',')\n",
    "\n",
    "...equivalent to...\n",
    "\n",
    "lambda x: x.split(',')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can map the `Genres` variable using `lambda x: x.split(',')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_genre = temp_genre.map(lambda x: x.split(','))\n",
    "temp_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reassign the original column to our manipulated Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Genres'] = temp_genre\n",
    "movies['Genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Country\n",
    "\n",
    "We can do the same for the `Country` variable. First, check how many null values there are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_country = movies[pd.isnull(movies['Country'])].copy()\n",
    "print(movies['Country'].isnull().sum())\n",
    "null_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "None, so we can proceed right away with copying the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_country = movies['Country'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the same mapping strategy of `lambda x: x.split(',')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_country = temp_country.map(lambda x: x.split(','))\n",
    "temp_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reassign the original column to our manipulated Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Country'] = temp_country\n",
    "movies['Country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Languages\n",
    "\n",
    "Finally, we'll repeat this with `Languages`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_lang = movies[pd.isnull(movies['Languages'])].copy()\n",
    "print(movies['Languages'].isnull().sum())\n",
    "null_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 4 movies with `NaN` in their `Languages` field. But do you notice anything? The first movie with sound was The Jazz Singer, released in 1927. All four of these movies were released before that year. So here's what we'll do...\n",
    "\n",
    "First, map the rest of the values as planned by setting `na_action='ignore'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lang = movies['Languages'].copy()\n",
    "\n",
    "temp_lang = temp_lang.map(lambda x: x.split(','), na_action='ignore')\n",
    "temp_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Languages'] = temp_lang\n",
    "movies['Languages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, filter to find all the movies made before 1927...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_films = movies[movies['Year'] < 1927].copy()\n",
    "silent_films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...and change their `Language` value to \"Silent\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silent_list = list(silent_films.index)\n",
    "\n",
    "for film in silent_list:\n",
    "    movies.loc[film, 'Languages'] = 'Silent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies['Year'] < 1927]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scaling Variables\n",
    "\n",
    "If we look at the three movie rating variables, each source has provided ratings for each movie on a different scale and in a different format. \n",
    "\n",
    "* `imdbRating`: 0.0-10.0; float format\n",
    "* `Metascore`: 0.0-100.0; float format\n",
    "* `Rotten Tomatoes`: 0-100%; string format\n",
    "\n",
    "For graphical comparisons, you always want numeric variables on the same scale. Since it's easier to see minute differences between data points on a larger scale, we'll scale `imdbRating` to match `Metascore` and eventually Rotten Tomatoes.\n",
    "\n",
    "First, how many movies are missing a rating from IMDb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbRating'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 5 null values, so we need to set `na_action='ignore'`, right? Nope! Here's the exception to `.map()`'s rule about null values. \n",
    "\n",
    "Assume `a = np.nan` (`np.nan` is the notation for a null value):\n",
    "\n",
    "* `a.split(',')` would raise an error because you can't apply that, or any, method or function to a null value\n",
    "* `a*10` will NOT raise an error because *basic mathematical operators* treat null values like 0s\n",
    "\n",
    "Knowing this, we could easily scale `imdbRating` with `movies['imdbRating']*10`, but let's use this opportunity to prove the null value exception with `.map()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbRating'] = movies['imdbRating'].map(lambda x: x*10)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Element-wise Functions with .apply()\n",
    "\n",
    "When applied to a Series object, the `.apply()` function is effectively the same as `.map()`. It's just another elementwise function. The difference is that you can pass it more complex functions (e.g. more than one line, conditionals, error handling, etc.), while `.map()` is mainly paired with simple lambda functions.\n",
    "\n",
    "* `s.apply()`\n",
    "\n",
    "As with `.map()`, if there are null values in the Series, an error will stop the code's execution. However, `.apply()` has no equivalent to the `na_action` parameter in `.map()`. If you don't want to drop all the rows with null values just to get your `.apply()` function working, you can **manually** skip over null values using the same logic behind the `na_action` parameter. For example, you can build in conditional logic or a try/except statement.\n",
    "\n",
    "### BONUS TOPIC: Row- & Column-wise Functions with .apply()\n",
    "\n",
    "For the purposes of cleaning the OMDb movies dataset, we only really need `.apply()` for editing individual columns. It's worth taking a small detour to at least mention that you can also use `.apply()` with dataframes. In this context, `.apply()` is a **row-wise** or **column-wise** function. Here's the difference:\n",
    "\n",
    "* **`s.apply(func)`** dynamically changes each value of a Series\n",
    "* **`df.apply(func, axis=0)`** dynamically changes each value *of each row/column* of a dataframe\n",
    "\n",
    "Of course, the `axis` parameter is what determines whether your function is row-wise or column-wise. However, it's a little counter-intuitive. We know that `axis 0` refers to rows and `axis 1` refers to columns, but in the context of `df.apply()`:\n",
    "\n",
    "* If `axis=0`, the objects passed to `func` will be *a Series containing the dataframe's COLUMNS*. The changes will be made to each value (i.e. column) in the set of columns.\n",
    "* If `axis=1`, the objects passed to `func` will be *a Series containing the dataframe's ROWS*. The changes will be made to each value (i.e. column) in the set of rows.\n",
    "\n",
    "### Reformat Runtime\n",
    "\n",
    "Down to business. Right now, the `Runtime` variable is in string format. If we want to include it in any quantitative analysis or even sort based on this column, we need the values to be numeric. Fixing this won't be as simple as typecasting because each value contains non-numeric characters.\n",
    "\n",
    "First, how many rows are missing data for `Runtime`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_runtime = movies[pd.isnull(movies['Runtime'])]\n",
    "print(len(missing_runtime))\n",
    "missing_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are only three, all of which are missing ratings from Rotten Tomatoes and Metascore. We might as well drop these rows, but we don't need to grab each one's index this time. (Flash back to when we removed TV shows using `movies.drop(labels=non_movie_ids, axis=0)`.) Since these three are the only rows with null values for `Runtime`, we can drop them as a group using `.dropna()`.\n",
    "\n",
    "* `df.dropna(axis=0, how='any', subset=[col1], inplace=False)`\n",
    "\n",
    "When you're dropping rows (i.e. axis=0), the `subset` parameters indicates which columns to check for null values. Accordingly, if you're checking for duplicates in multiple columns, the `how` parameters indicates whether you want the function to drop the row if `'any'` of those columns contain a null value or only if `'all'` of them are null.\n",
    "\n",
    "Let's drop all rows that contain a null value in the `Runtime` column from the `movies` dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(subset=['Runtime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Did it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Runtime'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Good. Now, we can make a temporary copy of the `Runtime` column for our `.apply()` operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_runtime = movies['Runtime'].copy()\n",
    "temp_runtime.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's where we define our custom function. The best way to approach this is to test the function on a single value. By the way, even though we just dropped the rows with null values, we should still build in logic to avoid null values from causing issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime_reformat(row):\n",
    "    \"\"\"remove min from str and convert field to int\"\"\"\n",
    "    try:\n",
    "        split_row = row.split(' ')\n",
    "        numeric_runtime = int(split_row[0])\n",
    "        return numeric_runtime\n",
    "    except Exception as e:\n",
    "        # if pd.isnull(row), error will occur\n",
    "        # print(e)\n",
    "        return row\n",
    "\n",
    "test = temp_runtime[0]\n",
    "result = runtime_reformat(test)\n",
    "\n",
    "# TESTING ONE VALUE...\n",
    "print(f'''\n",
    "BEFORE: {test}, {type(test)}\n",
    "AFTER: {result}, {type(result)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let it run on the whole Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_runtime = temp_runtime.apply(runtime_reformat)\n",
    "temp_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assign our freshly cleaned Series back to the `movies` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Runtime'] = temp_runtime\n",
    "movies['Runtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Filter/Drop Shorts\n",
    "\n",
    "In the last lesson, we dropped all TV shows from the dataframe because we only want to evaluate movies. In the same vein, it's not truly accurate to compare long-form movies to \"short-form videos\". That might include [animated shorts from Pixar](https://www.studiobinder.com/blog/pixar-shorts/), for example, or \"made-for-TV\" specials that last ~40-45 minutes (1 hour with commercials).\n",
    "\n",
    "How many \"shorts\" are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts = movies[movies['Runtime'] < 45].copy()\n",
    "shorts.sort_values(by=['Runtime'], ascending=False, inplace=True)\n",
    "\n",
    "print(len(shorts))\n",
    "shorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Drop these by grabbing their index labels and check to make sure they're gone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts_idx = list(shorts.index)\n",
    "movies.drop(labels=shorts_idx, axis=0, inplace=True)\n",
    "shorts = movies['Runtime'] < 45\n",
    "shorts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reformat imdbVotes\n",
    "\n",
    "`imdbVotes` needs to be a numeric variable as well, and we can likewise leverage the `.apply()` method on this Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imdbVotes = movies['imdbVotes'].copy()\n",
    "temp_imdbVotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All we need to do for the `imdbVotes` variable is remove the commas and typecast each value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def votes_reformat(row):\n",
    "    \"\"\"remove commas from str and convert field to int\"\"\"\n",
    "    try:\n",
    "        split_row = row.split(',')\n",
    "        votes = int(''.join(split_row))\n",
    "        return votes\n",
    "    except Exception as e:\n",
    "        # if pd.isnull(row), error will occur\n",
    "        # print(e)\n",
    "        return row\n",
    "\n",
    "test = temp_imdbVotes[0]\n",
    "votes_reformat(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The single-value test worked, so we'll run it on the whole column...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imdbVotes = temp_imdbVotes.apply(votes_reformat)\n",
    "temp_imdbVotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "...and then reassign `temp_imdbVotes` back to the `movies` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['imdbVotes'] = temp_imdbVotes\n",
    "movies['imdbVotes'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reformat Rotten Tomatoes\n",
    "\n",
    "Finally, we need to reformat `Rotten Tomatoes` ratings by simply stripping off the `%` character and typecasting it to a float. Technically, we can do this with `.map()`. We could use `lambda x: float(x.strip('%'))`. But instead, let's practice `.apply()` one more time! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_rt = movies['Rotten Tomatoes'].copy()\n",
    "temp_rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Write and test the custom function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_rt(row):\n",
    "    try:\n",
    "        stripped = float(row.strip('%'))\n",
    "        return stripped\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return row\n",
    "        \n",
    "test = temp_rt[0]\n",
    "print(test)\n",
    "strip_rt(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Apply the function to the whole `Rotten Tomatoes` Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_rt = temp_rt.apply(strip_rt)\n",
    "temp_rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reassign it back to the `movies` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Rotten Tomatoes'] = temp_rt\n",
    "movies['Rotten Tomatoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Here's our newly bright and shiny dataframe!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## New Functions Featured\n",
    "\n",
    "Functions featured include (in order of appearance):\n",
    "* `pd.to_numeric(s)`\n",
    "* `s.astype()`\n",
    "* `s.map(arg, na_action=None)`\n",
    "* `s.apply(func)`\n",
    "* `df.apply(func, axis=0)`\n",
    "* `df.dropna(axis=0, how='any', subset=[col1], inplace=False)`\n",
    "\n",
    "## ðŸ‹ï¸â€â™€ï¸ **EXERCISES** ðŸ‹ï¸â€â™€ï¸ \n",
    "\n",
    "*TBD*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
